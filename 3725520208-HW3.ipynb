{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61ab83ee",
   "metadata": {},
   "source": [
    "<h2>CSCI 544 - Applied Natural language processing Assignment-3</h2><br><br>\n",
    "<b>Name: </b>Anne Sai Venkata Naga Saketh <br>\n",
    "<b>USC Email: </b>annes@usc.edu<br>\n",
    "<b>USC ID: </b>3725520208<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5828b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import csv\n",
    "# Library for Hidden Markov Models (HMM)\n",
    "import hmmlearn\n",
    "# Library for creating defaultdicts, a subclass of dict\n",
    "from collections import defaultdict, Counter\n",
    "# Library providing functions that map Python operators to corresponding functions in the operator module\n",
    "import operator\n",
    "# Importing the JSON library to read and write the json files\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520f391",
   "metadata": {},
   "source": [
    "## Task 1: Vocabulary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dd642f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "1\tPierre\tNNP\n",
      "\n",
      "2\tVinken\tNNP\n",
      "\n",
      "Test data:\n",
      "1\tInfluential\n",
      "\n",
      "2\tmembers\n",
      "\n",
      "Dev data:\n",
      "1\tThe\tDT\n",
      "\n",
      "2\tArizona\tNNP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the training data\n",
    "with open('./data/train', 'r') as file:\n",
    "    train_data = file.readlines()\n",
    "\n",
    "# Print first two sentences for demonstration\n",
    "print(\"Train data:\")\n",
    "for sentence in train_data[:2]:  \n",
    "    print(sentence)\n",
    "    \n",
    "# Read the testing data\n",
    "with open('./data/test', 'r') as file:\n",
    "    test_data = file.readlines()\n",
    "\n",
    "# Print first two sentences for demonstration\n",
    "print(\"Test data:\")\n",
    "for sentence in test_data[:2]:  \n",
    "    print(sentence)\n",
    "    \n",
    "# Read the testing data\n",
    "with open('./data/dev', 'r') as file:\n",
    "    dev_data = file.readlines()\n",
    "\n",
    "# Print first two sentences for demonstration\n",
    "print(\"Dev data:\")\n",
    "for sentence in dev_data[:2]:  \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709867f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Vocabulary size after replacement of the least occurring occurrences by the threshold: 16920\n",
      "Total occurrences of ‘<unk>’ token are: 32537\n"
     ]
    }
   ],
   "source": [
    "# Set the threshold for word frequency\n",
    "frequency_threshold = 3\n",
    "\n",
    "# Initialize dictionaries to store word and tag frequencies, and count sentences\n",
    "w_f, t_f, s_c = defaultdict(int), defaultdict(int), 0\n",
    "\n",
    "# Initialize a list to store file data with start tokens\n",
    "f_d = [\"<s>\"]\n",
    "\n",
    "# Iterate through each line in the training data\n",
    "for line in train_data:\n",
    "    \n",
    "    # Check if the line is empty, indicating end of a sentence\n",
    "    if line.strip() == \"\":\n",
    "        \n",
    "        # Add start token to file data\n",
    "        f_d.append(\"<s>\")\n",
    "        \n",
    "        # Increment sentence count\n",
    "        s_c += 1  \n",
    "    else:\n",
    "        speech = line.strip().split(\"\\t\")\n",
    "        \n",
    "        # Check if the line has three parts (word, tag, frequency)\n",
    "        # Skip this line if it doesn't have three parts\n",
    "        if len(speech) != 3:\n",
    "            continue\n",
    "            \n",
    "        # Extract word and tag\n",
    "        w, t = speech[1], speech[2]\n",
    "        \n",
    "        # Increment word frequency\n",
    "        w_f[w] += 1\n",
    "        \n",
    "        # Increment tag frequency\n",
    "        t_f[t] += 1\n",
    "        \n",
    "        # Add line to file data\n",
    "        f_d.append(line.strip())  \n",
    "\n",
    "# Filter vocabulary based on frequency threshold\n",
    "vocabulary = {w: f for w, f in w_f.items() if f >= frequency_threshold}\n",
    "\n",
    "# Add <unk> token for infrequent words\n",
    "vocabulary[\"<unk>\"] = sum(f for w, f in w_f.items() if f < frequency_threshold)\n",
    "\n",
    "# Write vocabulary to a file\n",
    "with open(\"vocab.txt\", \"w\") as vocabulary_file:\n",
    "    # Write <unk> token with frequency\n",
    "    vocabulary_file.write(\"<unk>\\t0\\t{}\\n\".format(vocabulary[\"<unk>\"]))  \n",
    "    \n",
    "    # Write each word with its index and frequency to the file, sorted by frequency\n",
    "    sorted_vocabulary = sorted(vocabulary.items(), key=lambda x: x[1], reverse=True)\n",
    "    for idx, (w, f) in enumerate(sorted_vocabulary, start=1):\n",
    "        vocabulary_file.write(\"{}\\t{}\\t{}\\n\".format(w, idx, f))\n",
    "\n",
    "# Print vocabulary size after replacement and total occurrences of '<unk>'\n",
    "print(f\"The Vocabulary size after replacement of the least occurring occurrences by the threshold: {len(vocabulary)}\")\n",
    "print(f\"Total occurrences of ‘<unk>’ token are: {vocabulary['<unk>']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab8a2a",
   "metadata": {},
   "source": [
    "## Task 2: Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e168ac66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transition parameters in the hmm.json file are: 23373\n",
      "Number of emission parameters in the hmm.json file are: 1392\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries to store emission and transition probabilities\n",
    "e_probabilities, t_probabilities = defaultdict(int), defaultdict(int)\n",
    "\n",
    "# Initialize previous tag variable with start token\n",
    "prev_tag = \"<s>\"\n",
    "\n",
    "# Iterate through each line in the file data\n",
    "for line in f_d:\n",
    "    \n",
    "    # Check if the line is a start token\n",
    "    if line == \"<s>\":\n",
    "        prev_tag = \"<s>\"  # Reset previous tag to start token\n",
    "        continue\n",
    "\n",
    "    # Split the line by tab\n",
    "    speech = line.split(\"\\t\")  \n",
    "    w, cur_tag = speech[1], speech[2]  \n",
    "    \n",
    "    # Replace infrequent words with <unk>\n",
    "    w = w if w in vocabulary else \"<unk>\"\n",
    "    \n",
    "    # Increment emission probability\n",
    "    e_probabilities[(cur_tag, w)] += 1  \n",
    "\n",
    "    # Update transition probabilities based on previous and current tags\n",
    "    if prev_tag != \"<s>\":\n",
    "        t_probabilities[(prev_tag, cur_tag)] += 1\n",
    "    else:\n",
    "        \n",
    "        # Increment start transition probability\n",
    "        t_probabilities[(\"start\", cur_tag)] += 1  \n",
    "    prev_tag = cur_tag\n",
    "\n",
    "# Normalize emission probabilities by tag frequency\n",
    "for key in e_probabilities:\n",
    "    e_probabilities[key] /= t_f[key[0]]\n",
    "\n",
    "# Normalize transition probabilities by tag frequency or sentence count\n",
    "for key in t_probabilities:\n",
    "    if key[0] == \"start\":\n",
    "        t_probabilities[key] /= s_c\n",
    "    else:\n",
    "        t_probabilities[key] /= t_f[key[0]]\n",
    "\n",
    "# Print number of transition and emission parameters\n",
    "print(f\"Number of transition parameters in the hmm.json file are: {len(e_probabilities)}\")\n",
    "print(f\"Number of emission parameters in the hmm.json file are: {len(t_probabilities)}\")\n",
    "\n",
    "# Convert keys to string for emission and transition probabilities\n",
    "e_k = {f\"({tag},{word})\": prob for (tag, word), prob in e_probabilities.items()}\n",
    "t_k = {f\"({prev_tag},{next_tag})\": prob for (prev_tag, next_tag), prob in t_probabilities.items()}\n",
    "\n",
    "# Create an HMM model dictionary\n",
    "model = {\"Transition\": t_k, \"Emission\": e_k}\n",
    "\n",
    "# Save HMM model to a JSON file\n",
    "with open('hmm.json', 'w') as json_file:\n",
    "    json.dump(model, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2218e1",
   "metadata": {},
   "source": [
    "## Task 3: Greedy Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc19024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hmm_from_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Create a dictionary containing HMM parameters with keys \"Transition\" and \"Emission\"\n",
    "    return {\"Transition\": data[\"Transition\"], \"Emission\": data[\"Emission\"]}\n",
    "\n",
    "# Path to the JSON file containing HMM parameters\n",
    "hmm_file_path = 'hmm.json'\n",
    "\n",
    "# Load HMM parameters from the JSON file\n",
    "model = load_hmm_from_json(hmm_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f00628b",
   "metadata": {},
   "source": [
    "### Testing on the Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d32853e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file: greedy_dev.out has been generated successfully\n"
     ]
    }
   ],
   "source": [
    "# Specify the output file name\n",
    "output_file_name = \"greedy_dev.out\"\n",
    "\n",
    "# Initialize a list to store data to be written\n",
    "w_d = []\n",
    "\n",
    "# Initialize previous tag variable with start token\n",
    "prev_tag = \"start\"\n",
    "\n",
    "# Open the development data file for reading and output file for writing\n",
    "with open(output_file_name, \"w\") as output_file:\n",
    "    \n",
    "    # Iterate through each line in the development data\n",
    "    for line in dev_data:\n",
    "        w = line.split(\"\\t\")\n",
    "        \n",
    "        # Check if the line contains only one element, indicating start of a new sentence\n",
    "        if len(w) == 1:\n",
    "            \n",
    "            # Reset previous tag to start token\n",
    "            prev_tag = \"start\"  \n",
    "            \n",
    "            # Append line to write data\n",
    "            w_d.append(line)  \n",
    "            continue\n",
    "        else:\n",
    "            # Extract index and current word\n",
    "            idx, cur_word = w[0].strip(), w[1].strip()  \n",
    "            \n",
    "            # Replace infrequent words with <unk>\n",
    "            if cur_word not in vocabulary:\n",
    "                cur_word = \"<unk>\"\n",
    "            \n",
    "            # Initialize probability and temporary tag variables\n",
    "            prob_val, temp_tag = 0, \"\"  \n",
    "\n",
    "            # Iterate through each tag in the tag frequency dictionary\n",
    "            for tag_iter in t_f:\n",
    "                \n",
    "                # Check emission and transition probabilities for the current word and tag combination\n",
    "                e_c = (tag_iter, cur_word)\n",
    "                emission_prob_value = e_probabilities.get(e_c, 0)\n",
    "                t_c = (prev_tag, tag_iter)\n",
    "                transition_prob_value = t_probabilities.get(t_c, 0)\n",
    "                current_prob_val = emission_prob_value * transition_prob_value\n",
    "\n",
    "                # Update probability and temporary tag if current probability is higher\n",
    "                if current_prob_val >= prob_val:\n",
    "                    prob_val, temp_tag = current_prob_val, tag_iter\n",
    "                    \n",
    "            # Update previous tag with temporary tag\n",
    "            prev_tag = temp_tag\n",
    "            \n",
    "            # Construct the line to be written\n",
    "            cur_line = f\"{idx}\\t{cur_word}\\t{prev_tag}\\n\"\n",
    "            \n",
    "            # Append line to write data\n",
    "            w_d.append(cur_line)\n",
    "            \n",
    "    # Write all the data to the output file\n",
    "    output_file.writelines(w_d)\n",
    "\n",
    "# Printing the output\n",
    "print(\"Output file: greedy_dev.out has been generated successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484b614c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 131768, correct: 122390, accuracy: 92.88%\r\n"
     ]
    }
   ],
   "source": [
    "# !python eval.py -p greedy_dev.out -g ./data/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f130aeef",
   "metadata": {},
   "source": [
    "### Testing on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ba07564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file: greedy.out has been generated successfully\n"
     ]
    }
   ],
   "source": [
    "# Specify the output file name\n",
    "output_file_name = \"greedy.out\"\n",
    "\n",
    "# Initialize a list to store data to be written\n",
    "w_d = []\n",
    "\n",
    "# Initialize previous tag variable with start token\n",
    "prev_tag = \"start\"\n",
    "\n",
    "# Open the development data file for reading and output file for writing\n",
    "with open(output_file_name, \"w\") as output_file:\n",
    "    \n",
    "    # Iterate through each line in the development data\n",
    "    for line in test_data:\n",
    "        w = line.split(\"\\t\")\n",
    "        \n",
    "        # Check if the line contains only one element, indicating start of a new sentence\n",
    "        if len(w) == 1:\n",
    "            \n",
    "            # Reset previous tag to start token\n",
    "            prev_tag = \"start\"  \n",
    "            \n",
    "            # Append line to write data\n",
    "            w_d.append(line)  \n",
    "            continue\n",
    "        else:\n",
    "            # Extract index and current word\n",
    "            idx, cur_word = w[0].strip(), w[1].strip()  \n",
    "            \n",
    "            # Replace infrequent words with <unk>\n",
    "            if cur_word not in vocabulary:\n",
    "                cur_word = \"<unk>\"\n",
    "            \n",
    "            # Initialize probability and temporary tag variables\n",
    "            prob_val, temp_tag = 0, \"\"  \n",
    "\n",
    "            # Iterate through each tag in the tag frequency dictionary\n",
    "            for tag_iter in t_f:\n",
    "                \n",
    "                # Check emission and transition probabilities for the current word and tag combination\n",
    "                e_c = (tag_iter, cur_word)\n",
    "                emission_prob_value = e_probabilities.get(e_c, 0)\n",
    "                t_c = (prev_tag, tag_iter)\n",
    "                transition_prob_value = t_probabilities.get(t_c, 0)\n",
    "                current_prob_val = emission_prob_value * transition_prob_value\n",
    "\n",
    "                # Update probability and temporary tag if current probability is higher\n",
    "                if current_prob_val >= prob_val:\n",
    "                    prob_val, temp_tag = current_prob_val, tag_iter\n",
    "                    \n",
    "            # Update previous tag with temporary tag\n",
    "            prev_tag = temp_tag\n",
    "            \n",
    "            # Construct the line to be written\n",
    "            cur_line = f\"{idx}\\t{cur_word}\\t{prev_tag}\\n\"\n",
    "            \n",
    "            # Append line to write data\n",
    "            w_d.append(cur_line)\n",
    "            \n",
    "    # Write all the data to the output file\n",
    "    output_file.writelines(w_d)\n",
    "\n",
    "# Printing the output\n",
    "print(\"Output file: greedy.out has been generated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ba449",
   "metadata": {},
   "source": [
    "## Task 4: Viterbi Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1802b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(o_w, s_l, e_probabilities, t_probabilities):\n",
    "    # Calculate the total number of observations and states\n",
    "    tot_obs = len(o_w)\n",
    "    tot_s = len(s_l)\n",
    "    \n",
    "    # Initialize the Viterbi matrix and backpointers matrix\n",
    "    v_m = [[0 for _ in range(tot_s)] for _ in range(tot_obs)]\n",
    "    backtracking = [[0 for _ in range(tot_s)] for _ in range(tot_obs)]\n",
    "\n",
    "    # Initialization step\n",
    "    for ind in range(tot_s):\n",
    "        \n",
    "        # Calculate transition and emission probabilities for the first observation\n",
    "        t_prob = t_probabilities.get(('start', s_l[ind]), 1e-10)\n",
    "        e_prob = e_probabilities.get((s_l[ind], o_w[0]), 1e-10)\n",
    "        \n",
    "        # Initialize the first column of the Viterbi matrix and backpointers\n",
    "        v_m[0][ind] = t_prob * e_prob\n",
    "        backtracking[0][ind] = 0\n",
    "\n",
    "    # Recursion step\n",
    "    for t_s in range(1, tot_obs):\n",
    "        for ind in range(tot_s):\n",
    "            \n",
    "            # Calculate the maximum probability and the corresponding previous state\n",
    "            m_p, optimal_state = max(\n",
    "                (v_m[t_s-1][prev_state] * t_probabilities.get((s_l[prev_state], s_l[ind]), 1e-10) * e_probabilities.get((s_l[ind], o_w[t_s]), 1e-10), prev_state)\n",
    "                for prev_state in range(tot_s))\n",
    "\n",
    "            # Update the current cell in the Viterbi matrix and backpointers\n",
    "            v_m[t_s][ind] = m_p\n",
    "            backtracking[t_s][ind] = optimal_state\n",
    "\n",
    "    # Termination step\n",
    "    final_t_s = tot_obs - 1\n",
    "    # Find the final state with the highest probability\n",
    "    optimal_state = max(range(tot_s), key=lambda s: v_m[final_t_s][s])\n",
    "\n",
    "    # Path backtracking\n",
    "    minimal_route = [optimal_state]\n",
    "    for t_s in range(tot_obs - 1, 0, -1):\n",
    "        \n",
    "        # Insert the best previous state at the beginning of the optimal path\n",
    "        minimal_route.insert(0, backtracking[t_s][minimal_route[0]])\n",
    "\n",
    "    # Convert state indices to state labels and return the optimal path\n",
    "    return [s_l[region] for region in minimal_route]\n",
    "\n",
    "\n",
    "# Assuming emission_probs is a dictionary where keys are tuples (tag, word)\n",
    "# Extract unique tags from the keys of emission_probs\n",
    "states = set(tag for tag, _ in e_probabilities.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "250cf873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output has been written into the file viterbi_dev.out\n"
     ]
    }
   ],
   "source": [
    "# If 'states' was originally defined as a set, convert it to a list\n",
    "states = list(states)\n",
    "\n",
    "# Determine the output file based on the data type\n",
    "output_name = \"viterbi_dev.out\"\n",
    "\n",
    "viterbi_result = []\n",
    "\n",
    "idx_data, cw_data, o_w = [], [], []\n",
    "\n",
    "for line in dev_data:\n",
    "    \n",
    "    # Sentence boundary\n",
    "    if len(line.strip()) == 0: \n",
    "        \n",
    "        if o_w:\n",
    "            # Perform Viterbi decoding on observed words\n",
    "            wots = viterbi(o_w, states, e_probabilities, t_probabilities)\n",
    "            \n",
    "            # Append decoded tags along with word indices and original words\n",
    "            viterbi_result.extend(f\"{idx}\\t{word}\\t{tag}\\n\" for idx, word, tag in zip(idx_data, cw_data, wots))\n",
    "            \n",
    "        # Reset for next sentence\n",
    "        o_w, idx_data, cw_data = [], [], []\n",
    "        \n",
    "        viterbi_result.append(\"\\n\")\n",
    "        continue\n",
    "\n",
    "    idx, word = line.strip().split(\"\\t\")[:2]\n",
    "    idx_data.append(idx)\n",
    "    cw_data.append(word)\n",
    "    \n",
    "    # Replace out-of-vocabulary words with <unk>\n",
    "    o_w.append(word if word in vocabulary else \"<unk>\")\n",
    "\n",
    "# Write Viterbi output to file\n",
    "with open(output_name, \"w\") as viterbi_file:\n",
    "    viterbi_file.writelines(viterbi_result)\n",
    "    \n",
    "print(\"The output has been written into the file viterbi_dev.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aee24c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1\\tThat\\tDT' '38\\t.\\t.' 131751\r\n",
      "total: 131751, correct: 124384, accuracy: 94.41%\r\n"
     ]
    }
   ],
   "source": [
    "# !python eval.py -p viterbi_dev.out -g ./data/dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06fc5cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output has been written into the file viterbi.out\n"
     ]
    }
   ],
   "source": [
    "# If 'states' was originally defined as a set, convert it to a list\n",
    "states = list(states)\n",
    "\n",
    "# Determine the output file based on the data type\n",
    "output_name = \"viterbi.out\"\n",
    "\n",
    "viterbi_result = []\n",
    "\n",
    "idx_data, cw_data, o_w = [], [], []\n",
    "\n",
    "for line in test_data:\n",
    "    \n",
    "    # Sentence boundary\n",
    "    if len(line.strip()) == 0: \n",
    "        \n",
    "        if o_w:\n",
    "            # Perform Viterbi decoding on observed words\n",
    "            wots = viterbi(o_w, states, e_probabilities, t_probabilities)\n",
    "            \n",
    "            # Append decoded tags along with word indices and original words\n",
    "            viterbi_result.extend(f\"{idx}\\t{word}\\t{tag}\\n\" for idx, word, tag in zip(idx_data, cw_data, wots))\n",
    "            \n",
    "        # Reset for next sentence\n",
    "        o_w, idx_data, cw_data = [], [], []\n",
    "        \n",
    "        viterbi_result.append(\"\\n\")\n",
    "        continue\n",
    "\n",
    "    idx, word = line.strip().split(\"\\t\")[:2]\n",
    "    idx_data.append(idx)\n",
    "    cw_data.append(word)\n",
    "    \n",
    "    # Replace out-of-vocabulary words with <unk>\n",
    "    o_w.append(word if word in vocabulary else \"<unk>\")\n",
    "\n",
    "# Write Viterbi output to file\n",
    "with open(output_name, \"w\") as viterbi_file:\n",
    "    viterbi_file.writelines(viterbi_result)\n",
    "    \n",
    "print(\"The output has been written into the file viterbi.out\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
